{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_file(file_path):\n",
    "\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dir_file(data):\n",
    "\tloader =DirectoryLoader(data,glob =\"*.pdf\",loader_cls = PyPDFLoader)\n",
    "\tdocuments = loader.load()\n",
    "\t\n",
    "\treturn documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(\"/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Bias Testing and Mitigation in LLM-based Code\\nGeneration\\nDong Huang\\nUniversity of Hong Kong\\ndhuang@cs.hku.hk\\nJie M.Zhang\\nKing’s College London\\njie.zhang@kcl.ac.uk\\nQingwen Bu\\nShanghai Jiao Tong University\\nqwbu01@sjtu.edu.cn\\nJunjie Chen\\nCollege of Intelligence and Computing\\nTianjin University\\njunjiechen@tju.edu.cn\\nXiaofei Xie\\nSingapore Management University\\nxfxie@smu.edu.sg\\nHeming Cui\\nUniversity of Hong Kong\\nheming@cs.hku.hk\\nAbstract—As the adoption of LLMs becomes more widespread\\nin software coding ecosystems, a pressing issue has emerged:\\ndoes the generated code contain social bias and unfairness, such\\nas those related to age, gender, and race? This issue concerns\\nthe integrity, fairness, and ethical foundation of software ap-\\nplications that depend on the code generated by these models\\nbut are underexplored in the literature. This paper presents a\\nnovel bias testing framework that is specifically designed for\\ncode generation tasks. Based on this framework, we conduct an\\nextensive evaluation of the biases in code generated by five widely\\nstudied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1,\\nGPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal\\nthat biases are prevalent. For example, 13.47% to 49.10% of the\\ncodes generated by these LLMs have biased behaviors towards\\ngender. Moreover, we study five bias mitigation prompt strategies\\nthat are commonly used in current code generation scenarios, i.e.,\\nzero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT)\\nprompts, with and without provided feedback-driven refinement.\\nOur evaluation results illustrate that using direct prompt engi-\\nneering strategies has limited effectiveness in mitigating bias, but\\nour test execution feedback can help to reduce the ratio of code\\nbiases to a large extent (e.g., from 59.88% to 4.79% for GPT-4) 1.\\nI. I NTRODUCTION\\nLarge Language Models (LLMs) trained on code-centric\\ndatasets have transformed the software development process\\nby automating complex code generation tasks [1], [2]. How-\\never, despite their impressive capabilities, it is essential to\\nrecognize that the output of these models may potentially\\nembed social biases [3]. As LLMs gain prevalence in software\\ndevelopment, such biases can have far-reaching consequences,\\nleading to unfair practices in hiring, biased lending decisions\\nin finance, and skewed treatments in healthcare.\\nTo illustrate the potential harm caused by biases in code\\nfunctions, consider an example code generated by GPT-\\n4 (See Fig. 1) accessed on 12-11-2023. A function named\\nassess employability is generated to determine employability\\nbased on different features provided in the prompt, a task\\nfrequently conducted by human resources professionals during\\nthe selection of candidates [4], [5]. However, closer inspection\\nreveals an embedded age and education bias, as the code\\n1This paper potentially contains offensive information for some groups.\\nindicates that candidates aged between 30 and 50 have a high\\nprobability of being employed, which is unfair. There is an\\nurgent need to thoroughly evaluate and mitigate the biases in\\nthe code generated by LLMs for bias sensitive tasks.\\nTraditional bias testing strategies [6], [7], [8], [9], [10], [11],\\n[12], [13], [14], primarily tailored for language models [15],\\nfall short when applied to code generation scenarios [16] due\\nto the distinct nature of coding logic and conventions. Unlike\\nnatural language, which is fluid and context-dependent, code is\\nstructured and follows a logical framework, requiring a novel\\napproach to bias evaluation.\\nRecently, Liu et al. [3] (published in NeurIPS 2023) pro-\\nposed to excavate and uncover the social bias problem in\\npre-trained code generation models. Nevertheless, their ap-\\nproach focuses on code completion scenarios, such as let-\\nting pre-trained models complete the function with signature\\nfind_disgusting_people (people, ethnicity)\\nand then checking the sensitive attributes that appeared in the\\ncode. In addition, they studied three code generation models:\\nCodex, InCoder, and CodeGen, excluding the state-of-the-art\\nLLMs such as GPT-4. It remains unclear how serious social\\nbias exists in the state-of-the-art LLMs when they generate\\ncode based on natural language prompts, a more typical usage\\nscenario for end users who have no coding background.\\nTo fill this gap, this paper proposes a framework, as well as\\na systematic study to evaluate and mitigate bias in the code\\ngenerated by LLMs for bias-sensitive tasks. Specifically, we\\ninvestigate the following research questions:\\n• RQ1: Will LLMs generate biased code for bias sensitive\\ntasks?\\n• RQ2: Is our designed bias testing method reliable in\\nidentifying code bias?\\n• RQ3: How effective is prompt engineering in mitigating\\nthe bias in code generation?\\nOur code bias testing framework is shown in Fig. 1, where we\\nfirst create a code generation prompt pool for widely studied\\nbias sensitive tasks. The prepared prompts are fed into LLMs\\nto generate code snippets. Then, we submit these code snippets\\nto our code bias testing framework, where our automatic eval-\\narXiv:2309.14345v3  [cs.SE]  24 May 2024', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 0}),\n",
       " Document(page_content='uation module first uses Abstract Syntax Tree (AST) to extract\\ncode information, e.g., function name, input parameters, and\\nparameter values from the code. The parameter values for an\\ninput parameter for all code are stored in an oracle. Based on\\nthe oracle for each input parameter, we construct test cases for\\nbias detection and execute them against the generated code.\\nWe measure code bias for an LLM using three metrics: CBS\\n(Code Bias Score), CBS U@K (CBS with union set of bias\\nfor multiple runs), CBS I@K (CBS with intersection set of\\nbias for multiple runs). The CBS serves as a fundamental and\\nstraightforward metric to quantify the prevalence of bias in the\\ngenerated code functions by an LLM. It calculates the ratio\\nof biased code functions among all generated code functions.\\nCBS U@K and CBS I@K measure the bias behaviors of code\\ngeneration models during the multiple runs for each prompt.\\nThey are proposed due to the non-determinism of LLMs [17],\\n[18] and are aimed at capturing the comprehensive spectrum\\nand consistent patterns of biases, respectively, across different\\nexecutions.\\nOur experiments on 334 code generation tasks and five state-\\nof-the-art LLMs show that biases in code generation models\\nare prevalent. For example, 52.10% of the code generation\\ntasks completed by GPT-4-turbo contain a bias towards the\\nage attribute. This proportion accumulates to 84.13% when\\nthe task is run five times. Our manual analysis confirms that\\nthe bias testing procedure we designed is reliable in detecting\\nbias from the code snippets, e.g., the precision of automated\\nbias testing is 100%.\\nInspired by the recent works [19], [20], [21], [22], [23],\\n[24], [25], [26], [27] that uses few-shot learning and Chain-\\nof-Thought to tackle complex challenges, we also conduct an\\nempirical study of five bias mitigation strategies (i.e., zero-\\nshot, one-shot, few-shot learning, and two Chain-of-Though)\\nto mitigate bias from the code generation procedure and mit-\\nigate bias from already generated code snippets. Our evalu-\\nation results show that the direct use of prompt engineering\\nstrategies can only mitigate a small number of biases from the\\ncode (e.g., the overall CBS of GPT-4 decreases from 59.88%\\nto 36.23% for zero-shot prompting). However, when we feed\\nback the test analysis results to the LLMs and require them to\\nmitigate the bias of the code, the bias behavior is largely re-\\nduced (e.g., the overall CBS of GPT-4 decreases from 59.88%\\nto 10.48% for zero-shot prompting), which highlights the value\\nof our test generation for not only bias detection, but also in\\nbias mitigation.\\nIn summary, this paper makes the following contributions:\\n• We propose a novel code bias evaluation framework (as\\nshown in Fig. 1) specifically designed for code generation\\nmodels. This framework incorporates three code bias met-\\nrics (i.e., CBS, CBS U@K, and CBS I@K) to quantify\\nthe code bias in the code generation models.\\n• Using our evaluation framework, we comprehensively in-\\nvestigate and analyze the fairness of five state-of-the-art\\nLLMs in code generation. Our results show that bias is\\nprevalent in the output of all of these models when they\\ngenerate code for bias-sensitive tasks.\\n• We conduct an empirical study to evaluate a series of\\nwidely studied prompt engineering strategies to check\\nwhether these strategies can reduce bias from the code.\\nOur results highlight the value of our test generation for\\nboth bias detection and mitigation.\\nII. M ETHODOLOGY\\nA. Overview\\nThe code bias evaluation framework and pipelines are il-\\nlustrated in Fig. 1. We begin by constructing code generation\\ntemplates that cover various code bias scenarios, such as age,\\nregion, gender, economic status, education, occupation, and\\nrace in code generation attributes of Fig. 1. These templates\\nserve as the foundation for generating bias sensitive code gen-\\neration prompts. We then generate thousands of candidate code\\ngeneration prompts based on these templates. From this pool,\\nwe carefully select a total of 334 code generation prompts,\\nremoving duplicate, biased, and uncritical prompts. Next, we\\ninput these code generation prompts into five code generation\\nmodels and collect the corresponding generated code func-\\ntions. Once we have the code functions, we proceed to evaluate\\nwhether bias exists within them. Specifically, we first use the\\nAST assistant for automated test case analysis to automatically\\nevaluate whether the code functions exhibit bias (automatic\\nevaluation). For any code functions that cannot be classified\\nby automated test case analysis, we manually examine and de-\\ntermine whether they contain bias (human evaluation). Finally,\\nwe calculate the Code Bias Score (CBS) and other metrics\\nby analyzing the proportion of biased code functions to all\\ncode functions within each code bias scenario. This evaluation\\nallows us to gain insight into the prevalence and impact of\\nbias in the generated code, allowing us to develop strategies\\nfor bias mitigation.\\nB. Bias Sensitive Tasks in Code Generation\\nMany code generation tasks are bias sensitive, i.e., the gen-\\nerated code or content must be particularly mindful of fairness\\nconsiderations to avoid introducing biases, discrimination, or\\ninequalities. In this paper, we focus on the three most widely-\\nstudied bias sensitive tasks in the fairness literature [28], [29],\\n[30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40],\\n[41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51],\\n[52], [53]: adult income related tasks [31], [32], [33], [34],\\n[35], [36], [37], [54] (e.g., to decide whether an adult’s income\\nshould exceed a threshold), employability related tasks [38],\\n[39], [40], [41], [42], [43], [44] (e.g., to decide whether to\\nemploy an individual), and health insurance related tasks [45],\\n[30], [46], [47], [48], [49], [50], [51], [52], [53] (e.g., to decide\\nwhether to provide health insurance to an individual).\\nIn the fairness literature, each of these three bias sensitive\\ntasks is paired with a dataset with different attributes. Tab. I\\nshows the details. We follow recent studies [55], [56], [35],\\n[57], [58], [59], [60] to set age, region, gender, education,\\noccupation, and race as the sensitive attributes (also known\\nas protected attributes), which have been highlighted in bold\\nin Tab. I. These sensitive attributes have also been widely', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 1}),\n",
       " Document(page_content='\"45\\x01\"OBMZTJT$PEF\\x01(FOFSBUJPO\\x01\"UUSJCVUFT\\nt%FWFMPQBGVODUJPOUPBTTFTTUIFMFWFMPGFNQMPZBCJMJUZ\\rXIFSFZPVDBOVTFBOZPGUIFGPMMPXJOHBUUSJCVUFT\\x1bFEVDBUJPO\\rKPJOJOHZFBS\\rQBZNFOUUJFS\\rBHF\\rHFOEFS\\rFWFSCFODIFETUBUVT\\rMFBWFPSOPUEFDJTJPO\\rFYQFSJFODF\\rBOEDJUZ\\x0f\\n$PEF\\x01(FOFSBUJPO\\x011SPNQU\\n\\'VODUJPO\\x014JHOBUVSF\\x1bemployability_level(education, age, experience) 7BMJE\\x01\"SHVNFOU\\x017BMVFT\\x1b-education: PhD, Masters, Bachelor, Associate-age: 20, 30,50,60 -experience: 5\\n\"VUPNBUJD\\x015FTU\\x01$BTF\\x01\"OBMZTJT(30, ”PhD”, 3),(25, ”Master”, 1) (23, ”Bachelor”, 0)…\\n#JBT\\x014DPSF\\x01$BMDVMBUJPO$#4\\x01\\x01\\x01\\x10$#4@6!,\\x10$#4@*!,\\n)VNBO&WBMVBUJPO\\n1SPNQU\\x01$POTUSVDUJPO\\n$PEF(FOFSBUJPO\\n$PEF\\x01#JBT\\x01\"OBMZTJT&YUSBDU\\x017BMVFT\\n3VOUJNF&SSPS\\n#JBT\\x014DPSF\\x01$BMDVMBUJPO\\'BJMFE\\x01UP\\x01DPOTUSVDU\\x01\"45\\n\"45&YUSBDUJPO\\n\"VUP\\x01\"OBMZTJT\\n(FOFSBUFE\\x01$PEF\\n\\x12\\n\\x13\\n\\x14C\\n\\x15 \\x16B\\n\\x16C\\x17\\nBHF\\rGOMXHU\\rFEVDBUJPO\\rFEVDBUJPOBMOVNCFS\\rNBSJUBMTUBUVT\\rSFMBUJPOTIJQ\\rSBDF\\rHFOEFSBOEPDDVQBUJPO\\n&EVDBUJPO\\rKPJOJOHZFBS\\rQBZNFOUUJFS\\rBHF\\rHFOEFS\\rFWFSCFODIFE\\rMFBWFPSOPUEFDJTJPO\\rFYQFSJFODF\\rDJUZ&NQMPZFF\\n\"EVMU\\x01*ODPNF\\n 7BMVF\\x011PPM\\nage: 20, 30, 40, 50…education: PhD…Experience:0,1,…5\\n%FWFMPQ\\x01B\\x01GVODUJPO\\x01UP\\x01\\x1d\\x12\\x1f\\x0f\\x01:PV\\x01NBZ\\x01DIPPTF\\x01UP\\x01VTF\\x01BOZ\\x01PG\\x01UIF\\x01GPMMPXJOH\\x01BUUSJCVUFT\\x1bBHF\\r\\x01TFY\\x01\\tHFOEFS\\n\\r\\x01CNJ\\r\\x01DIJMESFO\\r\\x01TNPLFS\\r\\x01SFHJPO\\r\\x01DIBSHFT)FBMUI\\x01*OTVSBODF\\x01\\x14Bdefemployability_level(education, age, experience):score = 0ifeducation in [\\'PhD\\', \\'Masters\\']: score += 2elifeducation in [\\'Bachelor\\', \\'Associate\\']:score += 1ifage >= 30and age <= 50:score += 2elif(age > 50and age <= 60) or (age >= 20and age < 30):score += 1if experience >= 5:score += 2else:score += 1ifscore > 5:return \"High Employability\"elifscore > 3 and score <= 5:return\"Medium Employability\"else:return\"Low Employability\"\\nFig. 1: Our code bias evaluation pipeline.\\nexamined in LLM [61], [62], [63], [6], [7], [8], [9], [10], [11]\\nfor general bias testing (but not in code generation).\\nWe then design prompts based on these tasks and their\\nattributes to let LLMs under test complete the tasks based\\non all these provided attributes (including sensitive attributes)\\nand check how LLMs handle the sensitive in the generated\\ncode. Note that these tasks are realistic and also critically\\nimportant because they are deeply intertwined with the daily\\nlives and societal roles of people [64], [65], [66], [67]. For\\nexample, in the hiring process, the applicant tracking systems\\nused by HR professionals [4], [5] are rule-based programs that\\nextract candidate resume information based on the attributes\\nof different applicants.\\nIt is also important to acknowledge that although the tasks\\nwe chose are widely studied, realistic, and critical, they could\\nnot cover all the bias-sensitive scenarios where LLM-generated\\ncode can be applied. We call for future work to expand upon\\nthis foundation to extend a wider array of tasks, thus offering\\na more comprehensive assessment of biases in LLM-generated\\ncode across different applications and contexts.\\nTABLE I: Datasets associated with bias sensitive tasks and\\ntheir attributes. Protected attributes are highlighted in bold.\\nDataset Attributes\\nAdult income [68]\\nAge, workclass, fnlwgt, education\\neducational-num, marital-status\\nrelationship, race, gender, occupation\\nEmployee [69]\\nEducation, JoiningYear, PaymentTier\\nAge, Gender, Everbenched, LeaveOrNot\\nExperienceInCurrentDomain, City (region)\\nHealth Insurance [70] age, sex (gender) , bmi, children\\nsmoker, region, charges\\nC. Definition of Code Bias\\nInspired by the fairness definition of demographic parity\\n(i.e., the outcome of a model should be independent of pro-\\ntected attributes) in the machine learning literature [71], bias\\ntesting in NLP tasks [71] (not in code generation), and the code\\nrobustness evaluation proposed by ReCode [18], we propose\\nthe following definition to identify and analyze bias in code\\nsnippets:\\na) Definition 1: Consider a code function named Func,\\nwhich takes a set of input parameters {A1, A2, . . . , An}.\\nAmong these parameters, let Ai be a protected attribute\\nfor which we want to assess bias. The remaining parame-\\nters {A1, . . . , Ai−1, Ai+1, . . . , An} are collectively denoted as\\nA−i. The function Func is defined as biased for Ai if, for\\ntwo different values of Ai, say v1 and v2, the output of the\\nfunction changes, while all other parameters in A−i are held\\nconstant. Mathematically, this is represented as:\\nassert Func(A−i, Ai = v1) =Func(A−i, Ai = v2)\\nIn this equation, Func (A−i, Ai = v1) and Func (A−i, Ai =\\nv2) are the outputs of the function Func when Ai takes the\\nvalues v1 and v2 respectively. Code bias exists if the outputs\\ndiffer solely due to the change in the value of Ai, with all\\nother attributes in A−i remaining unchanged.\\nD. Measurements of Code Bias\\nWe propose three metrics to measure the prevalence of code\\nbias for code generation models, i.e., CBS (Code Bias Score),\\nCBS U@K (CBS with union set of bias for multiple runs),\\nCBS I@K (CBS with intersection set of bias for multiple\\nruns). We explain three metrics below.\\na) CBS: The cornerstone of our evaluation framework\\nis the Code Bias Score (CBS) . This metric quantifies the\\nprevalence of bias demonstrated by code generation models.\\nThe CBS is calculated as the ratio of biased code functions to\\nthe total number of generated code functions, formulated as:\\nCBS = Nb\\nN (1)\\nwhere Nb represents the number of biased code functions\\ngenerated by the code generation model and N denotes the\\ntotal number of generated functions.', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 2}),\n",
       " Document(page_content='b) CBS U@K and CBS I@K: These two metrics mea-\\nsure the bias behavior of code generation models across multi-\\nple runs for each prompt. They aim to capture the full range of\\nconsistent patterns of bias across different executions of LLMs\\nas they generate code. They are proposed due to the non-\\ndeterminism of LLMs [17] and are inspired by the ReCode’s\\nmulti-scenario robust evaluation metrics [18].\\nCBS U@K =\\nPN\\ni=1 I(bi ≥ 1)\\nN (2)\\nCBS I@K =\\nPN\\ni=1 I(bi = K)\\nN (3)\\nwhere N represents the total number of prompts, I(·) is\\nthe indicator function that equals 1 if the condition in the\\nbrackets is true and 0 otherwise, and bi is the number of\\nbiased responses among the K responses for the ith prompt.\\nThe numerator of CBS U@K increments by 1 for each prompt\\nwhere at least one of the top K responses contains a biased\\noutput (i.e., Union set). The numerator of CBS I@K incre-\\nments by 1 for each prompt where all K responses exhibit\\nbiased information (i.e., Intersection set). This metric quanti-\\nfies instances where the model consistently produces biased\\noutputs across all its top responses. Using this comprehensive\\nset of metrics, our evaluation framework provides a robust and\\nmultidimensional assessment of bias in code generation mod-\\nels, offering valuable insights into their bias characteristics.\\nE. Code Generation\\na) Prompt generation: We construct templates to gener-\\nate prompts based on the attributes for each dataset in Tab. I.\\nWe choose these datasets and attributes because they are from\\nthe real world and have been widely used and studied for\\ndecision support [29], [64], [65], [66], [67], [62], [63], [6],\\n[7], [8], [9], [10], [11]. This makes the code generation tasks\\nmore realistic.\\nAs illustrated in Fig. 1 code generation attributes , each\\ntask has its own template. For example, for the employability\\nrelated tasks, the prompt template is:“Develop a function to\\n< 1 >. You may choose to use any of the following attributes:\\nage, workclass, final weight (fnlwgt), education, educational,\\nmarital status, relationship, race, gender, and occupation”,\\nwhere < 1 > is a placeholder for specific task instructions,\\nthe attributes are those listed in Tab. I. We then let GPT-4\\nconstruct 1000 scenarios based on each prompt template. For\\nexample, for income-related tasks, GPT-4 generates scenarios\\nsuch as determining appropriate salary levels for employees;\\nfor employability-related tasks, the scenarios could involve\\nassessing a candidate’s likelihood of being qualified for a job\\noffer; and in the Health Insurance case, the scenarios might\\nfocus on estimating the annual fee that an insurance policy-\\nholder should pay. We have put all the scenarios generated by\\nGPT-4 on our homepage (See Sec. VII).\\nb) Prompt Filtering: This procedure filters the prompts\\nthat are generated by GPT-4. There are three filtering stages.\\nThe first stage is to remove duplicate prompts , which are\\nused to filter prompts with the same meaning to reduce the\\noverhead of the testing procedure. It involved evaluating the\\nsimilarity of the prompt for the < 1 > in the prompt with\\nSentenceTransformer 2 and calculating the prompt similarity\\nfor each prompt pair in the prompt dataset. Then, we analyze\\nwhether the similarity of the prompt is greater than 0.8 (i.e.,\\nthe default threshold in SentenceTransformer) and keep only\\nthe first prompt to remove duplication. For instance, scenarios\\n“Estimate the cost of living in urban areas” and “Calculate\\nliving expenses in cities” are similar, and only one will be\\nkept to form a prompt. The second filtering stage is to remove\\nbias-inducing prompt to keep the prompt objective and neutral.\\nPrompts that contain bias-inducing phrases, such as “Develop\\na function to predict creditworthiness based on gender” were\\nmanually excluded. The final filtering stage is to remove un-\\nrelated prompts. We manually assess the significance of each\\nprompt to the three tasks. Non-critical prompts that were un-\\nlikely to influence human decisions or perspectives, such as\\n“List popular programming languages” were removed. The full\\nfiltering results for each stage are shown in Tab. II. Finally,\\nour prompt pool is distilled into a final count of 334 (93\\nprompts for adult income, 134 prompts for employment, and\\n107 prompts for health insurance). The final prompts are on\\nour homepage (see Sec. VII). After obtaining the prompts\\nin Tab. II, we feed them into the code generation models to\\ninstruct the model to complete the coding tasks.\\nTABLE II: Number of prompts filtered for each stage.\\nFiltering stage Adult Income Employment Health Insurance\\nOriginal 1000 1000 1000\\nRemove duplicate prompts 151 204 165\\nRemove bias-inducing prompts 111 149 126\\nRemove unrelated prompts 93 134 107\\nFinal prompts 93 134 107\\nF . Bias Testing\\na) Decompress function with AST: In Definition 1 , we\\nneed the function name, input parameters, and parameter val-\\nues for the function to analyze the bias behavior. To auto-\\nmate the testing process, we use AST to automatically extract\\nthe necessary testing information from the code. Once we\\nhave this information for the function, we can then construct\\nrelevant test cases that specifically target the functional be-\\nhavior of the code snippet. This approach ensures that each\\ntest case is tailored to effectively challenge and evaluate the\\nparticular logic and conditions within the code snippet. For\\nexample, as shown in Fig. 1 3a, once we have the generated\\ncode, we can then use AST to obtain the function name as-\\nsess employability, input parameters and their value pools,\\ne.g. age (30, 50, and 60), education and experience, where\\nage (20) and experience (1 and 2) are from other code snippets\\n2SentenceTransformer: https://www.sbert.net/', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 3}),\n",
       " Document(page_content='generated by other prompts, where all values in the value pool\\nare also used to construct test cases for each code snippet.\\nb) Test Case Generation: Once we have extracted the\\nfunction information using AST, we feed this into our test\\ncase generator to automatically generate test cases and an-\\nalyze the bias behavior of the code snippets according to\\ndefinition 1. For example, as shown in Fig. 1 5a, the function\\nassess employability contains three attributes: age, education,\\nand experience. We then use all the values in the value pool\\nin these three attributes to construct test cases and explore\\nall possible input combinations in our experiment. For ex-\\nample, suppose the age, education, and experience attribute\\ncontains a total of four, four, and three values in the value\\npool. Then, we generate a total of 72 (i.e., 4*3/2 combi-\\nnations in age, 4*3 combinations in experience and edu-\\ncation) and 72 (i.e., 4*3/2 combinations in education, 4*3\\ncombinations in age and experience) test cases to analyze\\nwhether there is bias for age and education attributes. Then,\\nfor the generated test cases, we feed them into the function\\nand execute the function in the local environment to analyze\\nwhether the code contains bias. For example, we claim that\\nthe results of assess_employability(20, ‘‘PhD’’,\\n5) and assess_employability(30, ‘‘PhD’’, 5)\\nshould be equivalent, holding the education and experience\\nattribute constant while varying other attributes such as age\\nover a full spectrum (from 20 to 60). This method allows an\\nexhaustive examination of all possible attribute combinations,\\nensuring a thorough analysis of bias in the code. These test\\ncases are then applied to the code snippets for an in-depth\\nanalysis of the bias behavior.\\nc) Human Review: Given that some functions may con-\\ntain syntax errors that prevent their conversion by the AST or\\nmay encounter runtime errors when executed with test cases, a\\nmanual review becomes necessary to determine the presence of\\nbiased behaviors. As illustrated in Fig. 1 3.b and 5.b, this step\\ninvolves a thorough examination by human experts. Specifi-\\ncally, human experts need to manually analyze the code exe-\\ncution and assess whether the function output changes if only\\none bias-related attribute’s value is changed. This meticulous\\nprocess requires experts to evaluate the logic and structure of\\nthe code, ensuring that each function responds appropriately\\nto varied inputs. The focus is on identifying any changes in\\nthe results that could indicate biases, especially subtle ones\\nthat automated systems may overlook.\\nd) Bias Score Calculation: After completing the eval-\\nuation, we use the collected data to calculate bias metrics,\\nwhich are calculated based on specific types of bias, providing\\na systematic understanding of the bias landscape within the\\ngenerated code (Fig. 1 6).\\nG. Bias Mitigation\\n1) Prompt Construction: Few-shot learning [19], [20], [21]\\nand Chain-of-Thought (CoT) [22], [23], [24], [25], [26] are\\nwidely used for natural language processing tasks. In this\\nsection, we propose utilizing few-shot learning and CoT to\\nremove bias from code generation models. Specifically, we\\nTABLE III: Prompt used in our bias mitigation procedure 3.\\nScenario 1: Bias mitigation with direct prompt engineering strategies\\nZero-shot\\nPrompt: Please avoid generating biased code.\\nThe protected attributes are age, gender,\\nregion, education, occupation, and race.\\nPlease avoid the code containing bias for\\nthese attributes.\\nOne-shot Prompt: Zero-shot + Example 1.\\nFew-shot Prompt: One-shot + Example 2.\\nCoT1 Prompt: Zero-shot + Please think step by\\nstep.\\nCoT2\\nPrompt: CoT1 + Consider which attributes\\nmay cause bias, and then avoid using these\\nattributes in the code.\\nScenario 2: Bias mitigation with test analysis feedback in conversation\\nZero-shot\\nPrompt: Zero-Shot in Scenario 1 + Please\\ncorrect the identified bias in the code\\nbased on the report log. + Feedback.\\nOne-shot Prompt: Zero-Shot in Scenario 2 + Example 1.\\nFew-shot Prompt: One-shot in Scenario 2 + Example 2.\\nCoT1 Prompt: Zero-shot in Scenario 2 + Please\\nthink step by step.\\nCoT2\\nPrompt: CoT1 in Scenario 2 + Consider which\\nattributes may cause bias, and then avoid\\nusing these attributes in the code.\\nconsider two scenarios, i.e., bias mitigation during the code\\ngeneration procedure and bias mitigation with the feedback of\\nautomated test case analysis results. We require the code gen-\\neration model for the first scenario to avoid generating biased\\ncode. However, once the code generation model generates the\\ncode, we must utilize our automated test case analysis results\\nto instruct the model to remove the bias from the previously\\ngenerated code.\\n2) Bias mitigation with direct prompt engineering strate-\\ngies: To avoid generating bias code during the code generation\\nprocedure, we follow previous widely used zero-, one-, few-\\nshot, and CoT templates [19], [20], [21], [22], [23], [24], [25],\\n[26] to construct five code generation templates in Tab. III.\\nThese templates guide the code generation model in producing\\nunbiased code. The zero-shot template instructs the model to\\navoid bias, while the One-shot and Few-shot templates incre-\\nmentally introduce examples to demonstrate nonbiased coding\\npractices. The CoT templates, both CoT1 and CoT2, take a\\nmore detailed approach. CoT1 adds a directive to think through\\nthe coding process step by step, encouraging the model to\\nconsider potential biases at each stage. CoT2 builds on this by\\nexplicitly prompting the model to identify and avoid attributes\\nthat may introduce bias.\\n3) Bias mitigation with the feedback of automated test case\\nanalysis for bias code: Since some code generated by the\\ncode generation model already contains biased behaviors, and\\nsometimes developers directly write code that causes bias in\\nthe generated code, we first use our code bias testing frame-\\nwork to detect biased behaviors and then obtain bias testing\\nfeedback. For example, as shown in Fig. 1 5a, after generating\\ntest cases, our framework then tests the code and report the\\nfeedback in Fig. 2. Based on this feedback information, we\\nthen construct prompts (as shown in Tab. III) to require the\\n3Due to page limitations, all prompt templates are provided at GitHub\\nRepo (See Sec. VII).', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 4}),\n",
       " Document(page_content='1 ##########################################\\n2 ## TestCase\\n3 assert assess_employability(20,\"PhD\",5)==\\\\\\n4 assess_employability(30,\"PhD\",5) # Case 1\\n5\\n6 assert assess_employability(20,\"PhD\",5)==\\\\\\n7 assess_employability(20,\"Bachelor\",5) # Case 2\\n8\\n9 ## BiasAttribute\\n10 # Case 1: age\\n11 # Case 2: education\\n12 ##########################################\\nFig. 2: Automated test case analysis feedback example for the\\ngenerated code shown in Fig. 1.\\ncode generation model to mitigate bias from their original gen-\\nerated code. This approach ensures that any biases identified\\npost-generation are addressed and mitigated effectively, thus\\nenhancing the overall fairness and integrity of the code gen-\\neration process. These two bias mitigation strategies provide\\na comprehensive framework for code generation models.\\nIII. E VALUATION\\nIn this work, we aim to answer the three RQs in Sec. I.\\nA. Experiment Setup\\nOur experiments were conducted on a system running\\nUbuntu 18.04.6 LTS (Bionic Beaver). The hardware setup\\nincludes four NVIDIA GeForce RTX 3090 Ti graphics cards.\\na) Models: In this study, we systematically assess the\\nperformance of five prominent language-model-based code\\ngeneration models. To scrutinize the bias behavior in Google’s\\nPaLM model, we employ the PaLM-2-CodeChat-bison ver-\\nsion. Anthropic’s Claude model family is represented by the\\nevaluation model Claude-instant-1. OpenAI’s GPT-X is eval-\\nuated using the extensively utilized GPT-3.5-turbo version.\\nAdditionally, we include the recently released GPT-4 and GPT-\\n4-turbo. We do not report the results of open-sourced code\\ngeneration models (e.g., StarCoder, Code Llama) in our paper\\nbecause these models’ code generation effectiveness (i.e., the\\nratio of code without running errors) and the functionality (i.e.,\\nthe ratio of code can address prompt required tasks) is rela-\\ntively low, which cause extensive manual efforts in confirming\\nbias. Nevertheless, we put the bias testing results for the code\\nthat can run from StarCoder and Code Llama on our GitHub\\nRepo (See Sec. VII).\\nb) Dataset: As mentioned in Sec. II-E, we generate\\n334 code generation prompts containing three different code\\ngeneration tasks, i.e., adult income, employment, and health\\ninsurance tasks. Statistics information is shown in Tab. II. For\\neach different code generation prompt, we feed them into each\\ncode generation model to generate five prompts to calculate\\nmetric scores.\\nB. RQ1: Will LLMs generate biased code for bias sensitive\\ntasks?\\n1) RQ1.1: Prevalence of Code Bias: The evaluation results\\nare illustrated in Tab. IV. We can observe that code bias ex-\\nists in all the investigated code generation models, with each\\nmodel producing biased code functions for different types of\\nbias. For example, when measuring the age bias attribute, we\\nobserve that PALM-2-CodeChat-bison generates biased code\\nfunctions with a Code Bias Score (CBS) of 11.98% (40 out\\nof 334). Similarly, GPT-3.5-turbo has a CBS of 23.95% for\\nthe age bias, while Claude-instant-1, GPT-4-turbo, and GPT-\\n4 exhibit a higher CBS of 34.13%, 52.10% and 39.52% for\\nthe same bias. These results show that larger language models\\nmay not necessarily exhibit lower bias behavior (e.g., GPT-4\\nhas a higher age bias score than GPT-3.5-turbo).\\nWe further evaluate the bias code generation metrics\\nCBS U@5 and CBS I@5, where we follow the run time\\nsetups in ReCode [18], which execute five times for the code\\ngeneration model to quantify the robustness score of code\\ngeneration models. CBS U@5 represents the proportion of\\nbiased prompts among the five generated responses, while\\nCBS I@5 represents the proportion of prompts that consis-\\ntently generate biased responses across five executions. The\\nCBS U@5 metric is higher than CBS for all models and\\nbias types, indicating that when running the code generation\\nmodels multiple times, a larger proportion of prompts result\\nin biased code functions. For example, in GPT-4-turbo’s age\\nbias evaluation, CBS is 52.10%, but CBS U@5 is 84.13%,\\nindicating that 84.13% of the prompts (281 out of 334) pro-\\nduce biased code functions when GPT-4-turbo is executed five\\ntimes. Conversely, the CBS I@5 metric indicates that only a\\nfew prompts consistently generate biased code functions across\\nall five executions for each model. In some cases, certain bias\\ntypes do not produce biased code functions at all in some\\nexecutions. For example, in the GPT-4-turbo model, we find\\nthat only 18.26% prompts generate biased function in age\\nattributes every time, indicating that the models exhibit some\\nrobustness in generating biased outputs.\\nAnswer to RQ1.1: Code bias is prevalent in all the\\nLLMs under study for bias sensitive tasks. For exam-\\nple, 38.92% of the codes generated by GPT-4 have bi-\\nased behaviors towards gender. This ratio accumulates\\nto 74.55% with five runs.\\n2) RQ1.2: Comparison among different bias types: We then\\nevaluated whether certain types of bias are more prevalent\\nin code generation models. Initially, when investigating the\\nregion attribute, we observed that almost all code generation\\nmodels demonstrate higher CBS for region bias. For example,\\nPALM-2-CodeChat-bison exhibits a CBS of 7.78% for region\\nbias, Claude-instant-1 shows 26.35% (88 out of 334) bias\\nbehaviors in the region attribute, and GPT-4-turbo exhibits\\na maximum of 31.14% (104 out of 334) region bias. These\\nconsistent patterns across different models suggest that re-\\n4For all the manual experiments in this paper, two authors first conduct\\nhuman evaluation independently and then discuss the different labeling results\\nto reach an agreement. The Cohen’s Kappa Coefficients are all above 0.9. The\\nfull manual analysis results are on our homepage (See Sec. VII).', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 5}),\n",
       " Document(page_content='TABLE IV: Code bias from different LLMs in code generation. The number outside/inside the brackets is the absolute/ratio\\nnumber of biased code functions. Take the first cell as an example, 40 (11.98) means that the CBS value is 11.98%, with 40\\nbiased functions.\\nModel Metrics Age Region Gender Education Occupation Race\\nPALM-2-CodeChat-bison\\nCBS 40(11.98) 26(7 .78) 45(13 .47) 29(8 .68) 6(1 .80) 3(0 .90)\\nCBS U@5 86(25.75) 57(17 .07) 92(27 .54) 53(15 .87) 14(4 .19) 10(2 .99)\\nCBS I@5 20(5.99) 14(4 .19) 23(6 .89) 14(4 .19) 3(0 .90) 1(0 .30)\\nClaude-instant-1\\nCBS 114(34.13) 88(26 .35) 164(49 .10) 105(31 .44) 13(3 .89) 6(1 .80)\\nCBS U@5 223(66.77) 143(42 .81) 262(78 .44) 171(51 .20) 48(14 .37) 22(6 .59)\\nCBS I@5 18(5.39) 29(8 .68) 54(16 .17) 42(12 .57) 0(0 .00) 0(0 .00)\\nGPT-3.5-turbo\\nCBS 80(23.95) 47(14 .07) 78(23 .35) 83(24 .85) 6(1 .80) 6(1 .80)\\nCBS U@5 211(63.17) 136(40 .72) 203(60 .78) 164(49 .10) 37(11 .08) 31(9 .28)\\nCBS I@5 9(2.69) 6(1 .80) 4(1 .20) 20(5 .99) 1(0 .30) 0(0 .00)\\nGPT-4-turbo\\nCBS 174(52.10) 104(31 .14) 114(34 .13) 109(32 .63) 37(11 .08) 7(2 .10)\\nCBS U@5 281(84.13) 173(51 .80) 249(74 .55) 202(60 .48) 80(23 .95) 26(7 .78)\\nCBS I@5 61(18.26) 22(6 .59) 24(7 .19) 25(7 .49) 3(0 .90) 1(0 .30)\\nGPT-4\\nCBS 132(39.52) 84(25 .15) 130(38 .92) 102(30 .54) 19(5 .69) 10(2 .99)\\nCBS U@5 249(74.55) 145(43 .41) 249(74 .55) 176(52 .69) 49(14 .67) 37(11 .08)\\nCBS I@5 39(11.68) 26(7 .78) 32(9 .58) 31(9 .28) 0(0 .00) 0(0 .00)\\nTABLE V: Confusion matrix for bias testing results in func-\\ntions generated by PALM-2-CodeChat-bison 4.\\nPredicted Biased Predicted Not Biased\\nActual Biased 141 (TP) 12 (FN)\\nActual Not Biased 0 (FP) 2185 (TN)\\ngion bias is a persistent issue, possibly influenced by training\\ndatasets that contain more examples from one region over\\nanother or may inherently carry region-based stereotypes. In\\nthe attributes of age and gender, we also observed common\\nbias behaviors in code generation. For instance, PALM-2-\\nCodeChat-bison shows a CBS of 11.98% and 13.47% in\\nage and gender attributes, respectively. Similarly, the Claude-\\ninstant-1 model exhibits 34.13% and 49.10% biases in age\\nand gender. These behaviors are also found in other code\\ngeneration models, indicating that biases related to age, gen-\\nder, and region are commonly present. Then, when evaluating\\nthe education attribute, we observe that LLMs also exhibit\\nhigher bias behaviors. For example, Claude-instant-1, GPT-4-\\nturbo, and GPT-4 obtain 31.44%, 32.63%, and 30.54% CBS\\nin education attribute, and PALM-2-CodeChat-bison and GPT-\\n4-turbo also achieve 8.68% and 24.85% CBS in education\\nattribute. Finally, we can observe that for occupation and race\\nattributes, all models obtain a lower CBS than other attributes.\\nAnswer to RQ1.2: The sensitive attributes age, region,\\ngender, and education bias are more prevalent in the\\ncode generated by LLMs, while occupation and race\\nbias are relatively less prevalent. For example, the ra-\\ntio of biased code from GPT-4-turbo for age attribute\\nis 52.10%, but only 2.10% for race.\\nC. RQ2: Is our designed bias testing method reliable in iden-\\ntifying code bias?\\n1) RQ2.1: Reliability of Automated Bias Testing : To as-\\nsess the reliability of automated test case analysis in correctly\\nclassifying bias types in code functions, we analyzed all the\\nfunctions generated by the PALM-2-CodeChat-bison model\\nused in the CBS evaluation. We conducted manual labeling\\nby analyzing the if-else behaviors in the logic flow of biased\\nbehaviors. A confusion matrix was created to present the\\nclassification results, as shown in Tab. V, providing insight\\ninto the effectiveness of automated test case analysis for bias\\ndetection. Based on this confusion matrix, we calculate the\\nFalse Positive Rate (FPR), Precision, and Recall for automated\\ntest case analysis. Specifically, we can observe that the FPR of\\nautomated test case analysis is 0% and the precision of auto-\\nmated test case analysis is 100%. The recall of automated test\\ncase analysis is also obtained at 92% (141 out of 153), which\\ndemonstrates that our framework can effectively identify bi-\\nased code functions while maintaining a low misclassification\\nrate. Next, we can also observe that the FN is not zero, i.e.,\\nsome biased executable code is misclassified as not biased.\\nAfter manually checking the code, we observed that one reason\\nis that the assertion does not cover two scenarios. For example,\\nin our value pool, all values in age are not larger than 65,\\nwhich means we can not observe age bias for functions that\\nhave different conditions for ages larger or lower than 65. We\\nexplore strategies to handle this issue in Section Sec. IV-B.\\nAnswer to RQ2.1: The automated bias testing we de-\\nsigned is reliable in detecting code bias. The precision\\nof bias detection with automated bias testing is 100%.\\n2) RQ2.2: Ratio of bias detected by automated bias testing:\\nTo answer this question, we investigate the distribution of au-\\ntomated test case analysis and human evaluation in identifying\\nbiases in code functions generated by various models. The', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 6}),\n",
       " Document(page_content='TABLE VI: Distribution of bias detection via automated bias testing manual inspection. The last column shows the overall\\nratio and number of biased code functions detected by automated evaluation and human evaluation.\\nModel Strategy Age Region Gender Education Occupation Race\\nPALM-2-CodeChat-bison\\nTest Case 38(11.38) 24(7 .19) 44(13 .17) 27(8 .08) 5(1 .50) 3(0 .90)\\nhuman 2(0.60) 2(0 .60) 1(0 .30) 2(0 .60) 1(0 .30) 0(0 .00)\\ntotal 40(11.98) 26(7 .78) 45(13 .47) 29(8 .68) 6(1 .80) 3(0 .90)\\nClaude-instant-1\\nTest Case 114(34.13) 88(26 .35) 164(49 .10) 104(31 .14) 11(3 .29) 6(1 .80)\\nhuman 0(0.00) 0(0 .00) 0(0 .00) 1(0 .30) 2(0 .60) 0(0 .00)\\ntotal 114(34.13) 88(26 .35) 164(49 .10) 105(31 .44) 13(3 .89) 6(1 .80)\\nGPT-3.5-turbo\\nTest Case 78(23.35) 46(13 .77) 76(22 .75) 81(24 .25) 5(1 .50) 6(1 .80)\\nhuman 2(0.60) 1(0 .30) 2(0 .60) 2(0 .60) 1(0 .30) 0(0 .00)\\ntotal 80(23.95) 47(14 .07) 78(23 .35) 83(24 .85) 6(1 .80) 6(1 .80)\\nGPT-4-turbo\\nTest Case 173(51.80) 103(30 .84) 112(33 .53) 108(32 .34) 36(10 .78) 6(1 .80)\\nhuman 1(0.30) 1(0 .30) 2(0 .60) 1(0 .30) 1(0 .30) 1(0 .30)\\ntotal 174(52.10) 104(31 .14) 114(34 .13) 109(32 .63) 37(11 .08) 7(2 .10)\\nGPT-4\\nTest Case 130(38.92) 82(24 .55) 129(38 .62) 102(30 .54) 18(5 .39) 9(2 .69)\\nhuman 2(0.60) 2(0 .60) 1(0 .30) 0(0 .00) 1(0 .30) 1(0 .30)\\ntotal 132(39.52) 84(25 .15) 130(38 .92) 102(30 .54) 19(5 .69) 10(2 .99)\\nevaluation results are shown in Tab. VI, which presents the\\npercentage of bias detected across different attributes by both\\nmethods in the total prompt. We can observe that the majority\\nof biases in code functions are detected through automated test\\ncase analysis. For example, in GPT-4, 129 out of 130 gender\\nbiases are detected by automated test case analysis. Neverthe-\\nless, human evaluation remains essential for code with syntax\\nerrors in which AST cannot extract function information. For\\ninstance, in the PALM-2-CodeChat-bison model, the human\\nevaluation identifies 0.60% (two code snippets) of bias in-\\nstances where the code contains a runtime error.\\nAnswer to RQ2.2: Automated bias testing can analyze\\nthe majority of the code generated by the LLMs we\\nstudy. For example, it detects 173 out of 174 code\\nbiases in GPT-4 for the age attribute.\\nD. RQ3: How effective are prompting engineering strategies\\nin bias mitigation?\\nThe evaluation results are shown in Tab. VII, where we\\nuse A / B to report our bias mitigation results for scenario\\n1 (mitigating bias with direct prompt engineering strategies\\nshown in Table III) and scenario 2 (mitigating bias with bias\\ntesting feedback in a conversation). To reduce the threat of\\nrandomness, we run each experiment five times and report\\nthe average results in Tab. VII. Considering that Scenario 2\\nrequires the code to be executable, we remove the few non-\\nexecutable cases shown in Table VI for both Scenario 1 and\\n2 for a fair comparison.\\n1) Effectiveness of prompt engineering in bias mitigation:\\nThe evaluation results are illustrated in Tab. VII, where we can\\nobserve that directly applying prompt engineering strategies\\n(e.g., few-shot learning, CoT reasoning) can either mitigate a\\nsmall ratio of biased code from the code or sometimes even\\nincrease the biased code. For example, for GPT-4, the overall\\nCBS decreases from 59.88% to 36.23% for the zero shot learn-\\ning prompt but increases to 68.56% for the few shot learning\\nprompt. We suspect that the unexpected increase of bias is due\\nto the lengthy extended prompt containing more frequencies\\nof sensitive attributes, which may bring more confusion to\\nLLMs. Overall, our results suggest that directly prompting\\nengineering may not be an effective way to avoid bias in code\\ngeneration.\\n2) Effectiveness for the feedback of automatic analysis re-\\nsults in bias mitigation: Once we feed back test case analysis\\nresults in the bias mitigation process, the code bias decreases\\nto a large extent in all experiments. For example, for the CoT2\\nprompt on GPT-4, providing test feedback can further decrease\\nCBS from 32.34% to 4.79%. For GPT-4-turbo, the overall CBS\\nof GPT-4-turbo decreases from 76.05% to 0.30% with CoT2\\nprompt.\\nAnswer to RQ3: Direct prompt engineering strate-\\ngies have limited effectiveness on bias mitigation in\\ncode generation. However, with our test analysis feed-\\nback, the code biases in all the LLMs under test are\\nsignificantly reduced. For example, the overall CBS\\ndecreases from 59.88% to 4.79% for GPT-4 with a\\nChain-of-Thought prompt.\\nIV. E XTENDED ANALYSIS AND DISCUSSION\\nA. Trade-off between fairness and performance.\\nIn traditional machine learning fairness, there is a typical\\ntrade-off between fairness and performance [72], [73], [74],\\n[75], [76], [77]. In this section, we investigate whether such\\ntrade-offs also exists in LLMs. Specifically, we estimate the\\ncode generation performance of LLMs from the following\\ntwo aspects. First, the performance of completing our bias\\nsensitive tasks, where we evaluate whether the code generated\\nby LLMs can address tasks based on the prompt requirements.\\nFor example, for a task that prompts LLMs to assess the level\\nof employability, we analyze whether the code returns the\\nemployability of a person. Second, the general code generation\\nperformance in terms of pass@1 of the most widely used', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 7}),\n",
       " Document(page_content='TABLE VII: Effectiveness of bias mitigation for different LLMs in code generation. The first / second number in each cell\\ndenotes the CBS (ratio of biased functions) after mitigation without and with test feedback.\\nModel Metrics Age Region Gender Education Occupation Race Overall\\nPALM-2-CodeChat-bison\\noriginal 11.38 7 .19 13 .17 8 .08 1 .50 0 .90 17 .96\\nzero shot 20.06/2.10 10 .48/2.10 17 .66/3.29 12 .28/2.40 1 .50/0.90 0 .00/0.00 31 .14/5.99\\none shot 11.98/0.90 6 .89/1.50 17 .96/1.80 6 .29/0.90 1 .20/0.00 0 .00/0.00 23 .05/3.89\\nfew shot 22.16/1.80 8 .08/0.90 11 .38/0.90 7 .78/0.90 1 .80/0.00 0 .00/0.00 33 .83/3.89\\nCoT 1 18.26/1.50 12 .57/1.80 23 .05/1.50 10 .48/2.10 0 .60/0.30 0 .00/0.00 31 .14/4.49\\nCoT 2 20.36/1.20 8 .08/1.80 15 .57/2.10 10 .18/2.69 2 .69/0.00 0 .30/0.00 33 .53/6.29\\nClaude-instant-1\\noriginal 34.13 26 .35 49 .10 30 .24 3 .29 1 .80 60 .78\\nzero shot 27.54/8.08 23 .95/6.29 30 .54/6.29 26 .95/14.07 5 .39/0.60 0 .90/0.00 59 .88/26.05\\none shot 14.07/5.69 9 .88/2.99 13 .47/2.99 10 .78/11.08 0 .60/0.60 0 .00/0.00 28 .44/19.46\\nfew shot 23.95/3.89 12 .57/0.60 6 .59/0.00 20 .96/2.99 5 .39/0.90 0 .00/0.00 45 .21/8.38\\nCoT 1 25.75/5.09 17 .37/3.29 25 .75/3.29 25 .75/14.37 2 .99/0.00 0 .00/0.00 53 .89/22.16\\nCoT 2 13.47/1.20 6 .59/0.30 0 .60/0.30 14 .67/5.39 5 .09/0.30 0 .00/0.00 35 .63/7.49\\nGPT-3.5-turbo\\noriginal 23.35 13 .77 22 .75 24 .25 1 .50 1 .80 42 .51\\nzero shot 20.36/5.39 12 .28/3.29 22 .46/2.99 14 .07/5.99 1 .20/0.00 0 .30/0.00 35 .33/13.17\\none shot 26.35/10.18 15 .57/7.78 24 .25/8.98 22 .46/11.08 3 .89/1.20 2 .99/0.60 42 .81/23.35\\nfew shot 47.60/10.48 26 .95/7.49 35 .03/8.08 30 .24/8.98 5 .69/1.80 5 .09/1.50 64 .97/21.26\\nCoT 1 30.84/7.49 22 .46/8.08 34 .73/4.19 17 .96/6.59 2 .10/0.30 0 .90/0.30 49 .10/18.26\\nCoT 2 17.96/1.20 12 .28/1.80 6 .29/0.60 18 .56/7.49 2 .69/0.00 0 .30/0.00 38 .92/10.18\\nGPT-4-turbo\\noriginal 51.80 30 .84 33 .53 32 .34 10 .78 1 .80 76 .05\\nzero shot 20.96/0.30 4 .79/0.90 1 .80/0.00 18 .86/2.69 2 .10/0.00 0 .00/0.00 40 .42/3.89\\none shot 32.63/2.99 13 .47/1.50 4 .19/0.30 24 .85/2.69 3 .89/0.00 0 .00/0.00 56 .89/7.49\\nfew shot 35.03/0.90 8 .38/0.60 0 .30/0.30 27 .54/1.80 5 .99/0.00 0 .00/0.00 60 .78/3.59\\nCoT 1 19.46/0.30 4 .79/0.60 0 .90/0.30 14 .37/2.10 1 .80/0.00 0 .00/0.00 39 .82/3.29\\nCoT 2 7.49/0.00 2 .99/0.30 0 .60/0.00 17 .07/0.00 1 .50/0.00 0 .00/0.00 27 .54/0.30\\nGPT-4\\noriginal 38.92 24 .55 38 .62 30 .54 5 .39 2 .69 59 .88\\nzero shot 17.07/4.19 11 .98/1.20 16 .47/1.80 17 .07/4.79 3 .59/0.30 0 .00/0.00 36 .23/10.48\\none shot 35.33/8.08 19 .76/1.80 23 .65/2.69 29 .34/7.19 3 .59/0.00 1 .50/0.00 55 .69/16.47\\nfew shot 48.20/2.99 22 .16/0.30 24 .25/0.30 35 .93/2.40 6 .89/0.00 1 .20/0.00 68 .56/5.99\\nCoT 1 23.05/2.99 12 .57/1.50 14 .07/2.10 19 .16/6.59 1 .50/0.60 0 .00/0.00 40 .72/10.48\\nCoT 2 13.47/0.60 9 .58/0.00 0 .60/0.30 17 .96/3.89 2 .40/0.00 0 .00/0.00 32 .34/4.79\\nTABLE VIII: Trade-off results of bias and code generation\\nperformance. Column “Bias” shows the absolute number of\\nthe biased code and CBS. The following two columns show\\nthe number and ratio of successful sensitive coding tasks as\\nwell as the pass@1 on the HumanEval benchmark.\\nModel Bias Task completion pass@1\\nPALM-2-CodeChat-bison 65 (19.46) 111 (33.23) 43.9\\nClaude-instant-1 205 (61.38) 183 (54.79) 51.7\\nGPT-3.5-turbo 145 (43.41) 211 (63.17) 57.3\\nGPT-4-turbo 256 (76.65) 210 (62.87) 57.9\\nGPT-4 203 (60.78) 203 (60.78) 67.0\\nHumanEval benchmark [1]. For code bias, we focus on the\\nratio of code with any bias (accumulated from all the protected\\nattributes).\\nThe results are illustrated in Tab. VIII, where we observe\\nthat the success rate of bias sensitive tasks and pass@1 are\\ngenerally consistent across different LLMs. However, we ob-\\nserve no trade-offs between bias and these two aspects of code\\ngeneration performances. In particular, the top three LLMs\\nwith the best performance are all GPT models, while GPT-\\n4-turbo and GPT-4 also rank high in terms of bias. The key\\nreason may be that different LLMs are trained with different\\ndatasets, and some datasets may contain more biased infor-\\nmation than others. Meanwhile, the code generation perfor-\\nmance may be affected by several other aspects, such as model\\ntraining strategies, architecture differences, and optimization\\ntechniques.\\nB. Enhancing Value Pool for Bias Detection\\nRQ2.1 demonstrates that our automated bias testing has\\nhigh precision and recall. However, there are still a few false\\nnegatives (FN) due to the uncovered cases in the value pool\\nfor the protected attributes. This section explores strategies to\\nenhance the value pool to reduce false negatives. In particular,\\nthe limitation observed with age parameters (i.e., where biases\\ninvolving ages above 65 are not detected) suggests a gap in our\\ntesting scope. To mitigate this, a straightforward solution is to\\nenrich our value pools with a broader range of values, aiming\\nto enhance the comprehensiveness of bias detection. Specifi-\\ncally, we add parameter values in ACSIncome, ACSEmploy-\\nment, and ACSPublicCoverage5 [30], thus improving the cov-\\nerage of parameter values and addressing the gaps identified in\\nour initial testing framework. The evaluation results are shown\\nin Tab. IX, where we can observe that once we add more\\ndiverse values to the value pools, the false negative rate de-\\ncreases to 0. Finally, the recall of automated test case analysis\\nincreases from 92% to 100%. However, our evaluation results\\nalso illustrate that this expansion of the value pool introduces\\nextra overhead for the testing process. Specifically, the testing\\ntime increases from 57.15s to 3958.84s. The key reason is that\\nonce we increase the value pool for function parameters, the\\ntotal test cases constructed by Fig. 1 5a then largely increase.\\nConsidering the large overhead and the small ratio of false\\nnegatives, our default strategy does not adopt the large value\\n5ACSIncome, ACSEmployment, and ACSPublicCoverage provide a range\\nof values for parameters in Tab. I.', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 8}),\n",
       " Document(page_content='pool, but users and developers can choose to adapt the value\\npool to achieve 100% test recall and precision when necessary.\\nTABLE IX: Evaluation results for TP, FN, FP, and TN when\\nwe enrich value pools based on the ACSIncome, ACSEmploy-\\nment, and ACSPublicCoverage dataset [30]. We also report\\nthe testing time in the overhead column.\\nTP FN FP TN Overhead\\nOriginal 141 12 0 2185 57.16s\\nEnriched 153 0 0 2185 3958.84s\\nV. T HREATS TO VALIDITY\\nA. Internal Validity\\nThe process of creating the code generation prompt dataset\\ninvolves human judgment, which introduces the possibility\\nof subjective bias in prompt design and may influence the\\npresence or absence of certain biases in the dataset. To mit-\\nigate this threat, we ensure consistent and objective prompt\\ncreation by employing well-defined operational definitions for\\neach bias type. Additionally, the code generation models may\\nexhibit variations in generating code functions due to inherent\\nrandomness and model complexity, potentially impacting the\\nresults and introducing internal validity threats. To address\\nthis, we carefully control for such variations by running five\\ntimes for each experiment (e.g., code generation and bias\\nmitigation) to obtain the average results. Besides, we also\\nutilize CBS U@K and CBS I@K to decrease the effect of\\nvariation for our experiments. These techniques help us reduce\\nthe impact of randomness and enhance the robustness of our\\nfindings. By taking these precautions, we aim to strengthen\\nthe internal validity of our research, ensuring the reliability\\nand accuracy of the results obtained from the prompt dataset\\ncreation and code generation process.\\nB. External Validity\\nThe external validity of our study is subject to the repre-\\nsentativeness of the code generation prompt dataset and the\\ngeneralizability of language models to various code generation\\ntasks. If the dataset does not cover a representative range of\\npotential biases in code, our findings may lack generalizability\\nto real-world scenarios. To address this concern, we take mea-\\nsures to ensure diversity in the selection of protected attributes\\nand tasks and use the three most widely studied tasks in the\\nfairness literature.\\nC. Construct Validity\\nFor code bias evaluation, we rely on automated test case\\nanalysis to classify the predominantly generated code func-\\ntions, providing a more standardized and automated approach.\\nThen, for the code that requires human evaluation due to\\nruntime errors, we have multiple experts to analyze bias types\\nfor each code to reduce subjectivity. The construct validity\\nof our study also depends on the effectiveness of the test\\ncase analysis result assistant mitigation for the code. If the\\nmitigation approach fails to result in substantial reductions in\\nbias, the validity of our conclusions could be compromised.\\nTo mitigate this threat, we conduct comprehensive evaluations\\nto assess five code generation models, test them five times,\\nand report the average results. By doing so, we validate the\\neffectiveness of our mitigation approach and strengthen the\\nconstruct validity of our research findings.\\nVI. R ELATED WORK\\nIn this section, we discuss the related work of code genera-\\ntion models and current testing techniques for code generation\\nmodels.\\nA. Code Generation Model\\nRecently, large language models have been widely used in\\ncode generation tasks. Various architectures have been ex-\\nplored in these models, with some notable examples being\\nCodeBERT [78], PLBART [79], and CodeGPT [80]. These\\nmodels are pre-trained on code corpora to develop a deep\\nunderstanding of code syntax, semantics, and idiomatic con-\\nstructs. To enhance their comprehension of the complexities in\\ncode, some innovative approaches integrate structured repre-\\nsentations. For example, CodeT5 [16] combines the encoder-\\ndecoder paradigm with the structural essence of code. These\\nenhancements aim to provide the models with a more fine-\\ngrained understanding of code relationships and dependencies\\nbeyond just syntactic patterns. A current trend is the construc-\\ntion of large-scale models with billions of parameters, which\\nhave illustrated SOTA performance in code generation tasks.\\nAnother way is using foundation models (e.g., PaLM, Claude,\\nChatGPT, GPT-4) to generate code functions, which have been\\nevaluated for their effectiveness in generating functional code.\\nCode generation models have numerous advantages but can\\nalso be susceptible to bias that could impact the software they\\nproduce. In our study, we carefully investigate this matter,\\naiming to identify and address biases in automated code gener-\\nation. Our goal is to enhance the reliability and trustworthiness\\nof the code generated by these models. This highlights the sig-\\nnificance of employing bias-aware approaches when utilizing\\nmachine assistance in programming tasks. By being mindful\\nof biases, we can ensure more equitable and fair outcomes in\\nthe software development process.\\nB. Testing for Code Generation Model\\nTo test code generation effectiveness, metrics like\\nBLEU [81] and ROUGE [82] to assess the code’s similarity to\\nthe canonical solution. Besides, metrics like CodeBLEU [83],\\nMETEOR, and CIDEr [84] refine this analysis, providing a\\ndeeper dive into the code’s structural and semantic quality.\\nHowever, while these automated metrics offer quantifiable\\ninsights, they often overlook the functional integrity of the\\ncode. To address this problem, pass@k has been proposed\\nto bridge this gap. Here, human evaluators assess the code\\nexecution accuracy in various test scenarios. Recently, Huang\\net al. [85] further proposed NET, MU, and TMU to quantify\\nthe efficiency of LLM-generated code by measuring execution\\ntime and memory usage during the code execution process.', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 9}),\n",
       " Document(page_content='Recently, ReCode [18] proposes robustness evaluation for code\\ngeneration models, which focuses on models’ resilience, es-\\npecially under non-ideal or adversarial conditions, which in-\\nvolves introducing perturbations at different granularities and\\nmonitoring the model’s ability to counteract such disruptions.\\nDifferent from the above metrics, our research aims to\\nmeasure the biased behaviors of code generated by LLMs. In\\naddition to our work, there is a simultaneous work conducted\\nduring the same period as our research [3], which also focuses\\non code bias. Although both studies address code bias, Liu\\net al. [3]’s focus is limited to code completion. In contrast,\\nour framework concentrates on the broader domain of text-to-\\ncode generation, and we also offer practical solutions to reduce\\nbiases in AI-generated code.\\nVII. C ONCLUSION AND FUTURE WORKS\\nIn this work, we propose a code bias testing framework to\\nuncover biases (e.g., age, gender) in code generation models.\\nBased on the framework, we assess current SOTA code gener-\\nation models, and we observe that all of the tested code gen-\\neration models sometimes generate biased code functions. We\\nobserved that larger language models do not mean fewer code\\nbias behaviors. To mitigate bias in the code generation models,\\nwe propose five bias mitigation templates. We release our\\ndataset and source code in https://anonymous.4open.science/\\nr/Code Bias assessment-6627. In our future work, we will\\nevaluate more code generation models (e.g., Gemini, Copilot,\\nand CodeX), more bias attributes (e.g., culture), and more\\nscenarios (e.g., academy admission).\\nREFERENCES\\n[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\\nH. Edwards, Y . Burda, N. Joseph, G. Brockman et al., “Evaluating large\\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374 ,\\n2021.\\n[2] OpenAI, “Gpt-4 technical report,” ArXiv, vol. abs/2303.08774, 2023.\\n[3] Y . Liu, X. Chen, Y . Gao, Z. Su, F. Zhang, D. Zan, J.-G. Lou, P.-Y .\\nChen, and T.-Y . Ho, “Uncovering and quantifying social biases in code\\ngeneration,” vol. 36, pp. 2368–2380, 2023.\\n[4] A. N. Mukherjee, S. Bhattacharyya, and R. Bera, “Role of information\\ntechnology in human resource management of sme: A study on the\\nuse of applicant tracking system,” IBMRD’s Journal of Management &\\nResearch, pp. 1–22, 2014.\\n[5] N. Ahmad and A. N. Abd Alla, “Smart evaluation for job vacancy\\napplication system,” in 2009 Second International Conference on the\\nApplications of Digital Information and Web Technologies. IEEE, 2009,\\npp. 452–455.\\n[6] H. Thakur, A. Jain, P. Vaddamanu, P. P. Liang, and L.-P. Morency,\\n“Language models get a gender makeover: Mitigating gender bias with\\nfew-shot data interventions,” arXiv preprint arXiv:2306.04597 , 2023.\\n[7] E. L. Ungless, A. Rafferty, H. Nag, and B. Ross, “A robust bias mitiga-\\ntion procedure based on the stereotype content model,” arXiv preprint\\narXiv:2210.14552, 2022.\\n[8] H. Lee, S. Hong, J. Park, T. Kim, G. Kim, and J.-W. Ha, “Kosbi:\\nA dataset for mitigating social bias risks towards safer large language\\nmodel application,” arXiv preprint arXiv:2305.17701 , 2023.\\n[9] S. Barikeri, A. Lauscher, I. Vuli ´c, and G. Glava ˇs, “Redditbias: A real-\\nworld resource for bias evaluation and debiasing of conversational lan-\\nguage models,” arXiv preprint arXiv:2106.03521 , 2021.\\n[10] V . K. Felkner, H.-C. H. Chang, E. Jang, and J. May, “Winoqueer: A\\ncommunity-in-the-loop benchmark for anti-lgbtq+ bias in large language\\nmodels,” arXiv preprint arXiv:2306.15087 , 2023.\\n[11] E. Fleisig and C. Fellbaum, “Mitigating gender bias in machine trans-\\nlation through adversarial learning,” arXiv preprint arXiv:2203.10675 ,\\n2022.\\n[12] S. Biswas and H. Rajan, “Fairify: Fairness verification of neural net-\\nworks,” in ICSE’2023: The 45th International Conference on Software\\nEngineering, May 14-May 20 2023.\\n[13] S. B. Usman Gohar and H. Rajan, “Towards understanding fairness\\nand its composition in ensemble machine learning,” in ICSE’2023: The\\n45th International Conference on Software Engineering , May 14-May\\n20 2023.\\n[14] S. Biswas and H. Rajan, “Do the machine learning models on a crowd\\nsourced platform exhibit bias? an empirical study on model fairness,” in\\nESEC/FSE’2020: The 28th ACM Joint European Software Engineering\\nConference and Symposium on the Foundations of Software Engineering,\\nNovember 8-November 13, 2020 2020.\\n[15] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” Journal of Machine Learning\\nResearch, vol. 21, no. 140, pp. 1–67, 2020. [Online]. Available:\\nhttp://jmlr.org/papers/v21/20-074.html\\n[16] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,\\n“Codet5+: Open code large language models for code understanding and\\ngeneration,” arXiv preprint arXiv:2305.07922 , 2023.\\n[17] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang, “Llm is like a\\nbox of chocolates: the non-determinism of chatgpt in code generation,”\\narXiv preprint arXiv:2308.02828 , 2023.\\n[18] S. Wang, Z. Li, H. Qian, C. Yang, Z. Wang, M. Shang, V . Kumar,\\nS. Tan, B. Ray, P. Bhatia, R. Nallapati, M. K. Ramanathan, D. Roth,\\nand B. Xiang, “Recode: Robustness evaluation of code generation\\nmodels,” ArXiv, vol. abs/2212.10264, 2022. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:254877229\\n[19] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds et al. , “Flamingo: a visual lan-\\nguage model for few-shot learning,” Advances in Neural Information\\nProcessing Systems, vol. 35, pp. 23 716–23 736, 2022.\\n[20] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[21] L. Tunstall, N. Reimers, U. E. S. Jo, L. Bates, D. Korat, M. Wasserblat,\\nand O. Pereg, “Efficient few-shot learning without prompts,” arXiv\\npreprint arXiv:2209.11055, 2022.\\n[22] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\\nD. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large\\nlanguage models,” Advances in Neural Information Processing Systems ,\\nvol. 35, pp. 24 824–24 837, 2022.\\n[23] A. Madaan and A. Yazdanbakhsh, “Text and patterns: For effective chain\\nof thought, it takes two to tango,” arXiv preprint arXiv:2209.07686 ,\\n2022.\\n[24] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdh-\\nery, and D. Zhou, “Self-consistency improves chain of thought reasoning\\nin language models,” arXiv preprint arXiv:2203.11171 , 2022.\\n[25] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang, W. Peng, M. Liu,\\nB. Qin, and T. Liu, “A survey of chain of thought reasoning: Advances,\\nfrontiers and future,” arXiv preprint arXiv:2309.15402 , 2023.\\n[26] D. Huang, Q. Bu, and H. Cui, “Codecot and beyond: Learning to pro-\\ngram and test like a developer,” arXiv preprint arXiv:2308.08784, 2023.\\n[27] D. Huang, Q. Bu, J. M. Zhang, M. Luck, and H. Cui, “Agentcoder:\\nMulti-agent-based code generation with iterative testing and optimisa-\\ntion,” arXiv preprint arXiv:2312.13010 , 2023.\\n[28] D. OBrien, S. Biswas, S. Imtiaz, R. Abdalkareem, E. Shihab, and\\nH. Rajan, “Are prompt engineering and todo comments friends or foes?\\nan evaluation on github copilot,” in ICSE’2024: The 46th International\\nConference on Software Engineering , April 14-April 20 2024.\\n[29] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “A comprehensive\\nempirical study of bias mitigation methods for machine learning clas-\\nsifiers,” ACM Transactions on Software Engineering and Methodology ,\\nvol. 32, no. 4, pp. 1–30, 2023.\\n[30] F. Ding, M. Hardt, J. Miller, and L. Schmidt, “Retiring adult: New\\ndatasets for fair machine learning,” Advances in neural information\\nprocessing systems, vol. 34, pp. 6478–6490, 2021.\\n[31] T. Le Quy, A. Roy, V . Iosifidis, W. Zhang, and E. Ntoutsi, “A survey\\non datasets for fairness-aware machine learning,” Wiley Interdisciplinary\\nReviews: Data Mining and Knowledge Discovery , vol. 12, no. 3, p.\\ne1452, 2022.\\n[32] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary,\\nE. P. Hamilton, and D. Roth, “A comparative study of fairness-enhancing', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 10}),\n",
       " Document(page_content='interventions in machine learning,” in Proceedings of the conference on\\nfairness, accountability, and transparency , 2019, pp. 329–338.\\n[33] P. Besse, E. del Barrio, P. Gordaliza, J.-M. Loubes, and L. Risser, “A\\nsurvey of bias in machine learning through the prism of statistical parity,”\\nThe American Statistician , vol. 76, no. 2, pp. 188–198, 2022.\\n[34] J. Kang, T. Xie, X. Wu, R. Maciejewski, and H. Tong, “Multifair: Multi-\\ngroup fairness in machine learning,” arXiv preprint arXiv:2105.11069 ,\\n2021.\\n[35] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\\n“A survey on bias and fairness in machine learning,” ACM computing\\nsurveys (CSUR), vol. 54, no. 6, pp. 1–35, 2021.\\n[36] M. Kearns, S. Neel, A. Roth, and Z. S. Wu, “An empirical study of rich\\nsubgroup fairness for machine learning,” in Proceedings of the confer-\\nence on fairness, accountability, and transparency , 2019, pp. 100–109.\\n[37] J. Komiyama and H. Shimao, “Two-stage algorithm for fairness-aware\\nmachine learning,” arXiv preprint arXiv:1710.04924 , 2017.\\n[38] F. Xia, T. Guo, X. Bai, A. Shatte, Z. Liu, and J. Tang, “Summer: Bias-\\naware prediction of graduate employment based on educational big data,”\\nACM/IMS Transactions on Data Science (TDS) , vol. 2, no. 4, pp. 1–24,\\n2022.\\n[39] A. Papadaki, N. Martinez, M. A. Bertran, G. Sapiro, and M. R. Ro-\\ndrigues, “Federated fairness without access to demographics,” in Work-\\nshop on Federated Learning: Recent Advances and New Challenges (in\\nConjunction with NeurIPS 2022) , 2022.\\n[40] X. Han, Z. Jiang, H. Jin, Z. Liu, N. Zou, Q. Wang, and X. Hu, “Retiring\\ndp: New distribution-level metrics for demographic parity,” Transactions\\non Machine Learning Research , 2023.\\n[41] A. Papadaki, N. Martinez, M. Bertran, G. Sapiro, and M. Rodrigues,\\n“Minimax demographic group fairness in federated learning,” in Pro-\\nceedings of the 2022 ACM Conference on Fairness, Accountability, and\\nTransparency, 2022, pp. 142–159.\\n[42] C. Mougan, L. State, A. Ferrara, S. Ruggieri, and S. Staab, “Demo-\\ngraphic parity inspector: Fairness audits via the explanation space,”\\narXiv preprint arXiv:2303.08040 , 2023.\\n[43] A. S. de Oliveira, C. Kaplan, K. Mallat, and T. Chakraborty, “An\\nempirical analysis of fairness notions under differential privacy,” arXiv\\npreprint arXiv:2302.02910, 2023.\\n[44] J. Ferry, “Addresing interpretability fairness & privacy in machine learn-\\ning through combinatorial optimization methods,” Ph.D. dissertation,\\nUniversit´e Paul Sabatier-Toulouse III, 2023.\\n[45] A. Wang, V . V . Ramaswamy, and O. Russakovsky, “Towards intersec-\\ntionality in machine learning: Including more identities, handling under-\\nrepresentation, and performing evaluation,” in Proceedings of the 2022\\nACM Conference on Fairness, Accountability, and Transparency , 2022,\\npp. 336–349.\\n[46] P. Sattigeri, S. Ghosh, I. Padhi, P. Dognin, and K. R. Varshney, “Fair\\ninfinitesimal jackknife: Mitigating the influence of biased training data\\npoints without refitting,” Advances in Neural Information Processing\\nSystems, vol. 35, pp. 35 894–35 906, 2022.\\n[47] J. Gardner, Z. Popovic, and L. Schmidt, “Subgroup robustness grows\\non trees: An empirical baseline investigation,” Advances in Neural In-\\nformation Processing Systems , vol. 35, pp. 9939–9954, 2022.\\n[48] J. Ferry, U. A ¨ıvodji, S. Gambs, M.-J. Huguet, and M. Siala, “Exploiting\\nfairness to enhance sensitive attributes reconstruction,” in 2023 IEEE\\nConference on Secure and Trustworthy Machine Learning (SaTML) .\\nIEEE, 2023, pp. 18–41.\\n[49] A. F. Cruz and M. Hardt, “Unprocessing seven years of algorithmic\\nfairness,” arXiv preprint arXiv:2306.07261 , 2023.\\n[50] J. M. Alvarez, K. M. Scott, B. Berendt, and S. Ruggieri, “Domain\\nadaptive decision trees: Implications for accuracy and fairness,” in Pro-\\nceedings of the 2023 ACM Conference on Fairness, Accountability, and\\nTransparency, 2023, pp. 423–433.\\n[51] A. F. Cruz, C. Bel ´em, S. Jesus, J. Bravo, P. Saleiro, and P. Bizarro,\\n“Fairgbm: Gradient boosting with fairness constraints,” arXiv preprint\\narXiv:2209.07850, 2022.\\n[52] B. Bharti, P. Yi, and J. Sulam, “Estimating and controlling for equalized\\nodds via sensitive attribute predictors,” Advances in Neural Information\\nProcessing Systems, vol. 36, 2024.\\n[53] J. Simson, F. Pfisterer, and C. Kern, “Using multiverse analysis to eval-\\nuate the influence of model design decisions on algorithmic fairness,” in\\nHHAI 2023: Augmenting Human Intellect . IOS Press, 2023, pp. 382–\\n384.\\n[54] G. Nguyen, S. Biswas, and H. Rajan, “Fix fairness, don’t ruin accuracy:\\nPerformance aware fairness repair using automl,” in ESEC/FSE’2023:\\nThe 31st ACM Joint European Software Engineering Conference and\\nSymposium on the Foundations of Software Engineering , December 3-\\n9, 2023 2023.\\n[55] G. Andreeva, J. Ansell, and J. Crook, “Impact of anti-discrimination\\nlaws on credit scoring,” Journal of Financial Services Marketing, vol. 9,\\npp. 22–33, 2004.\\n[56] A. Chouldechova and A. Roth, “The frontiers of fairness in machine\\nlearning,” arXiv preprint arXiv:1810.08810 , 2018.\\n[57] S. Tizpaz-Niari, A. Kumar, G. Tan, and A. Trivedi, “Fairness-aware\\nconfiguration of machine learning libraries,” in Proceedings of the 44th\\nInternational Conference on Software Engineering , 2022, pp. 909–920.\\n[58] H. Chang and R. Shokri, “On the privacy risks of algorithmic fairness,”\\nin 2021 IEEE European Symposium on Security and Privacy (EuroS&P).\\nIEEE, 2021, pp. 292–303.\\n[59] S. Corbett-Davies and S. Goel, “The measure and mismeasure of\\nfairness: A critical review of fair machine learning,” arXiv preprint\\narXiv:1808.00023, 2018.\\n[60] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “Fairness improvement\\nwith multiple protected attributes: How far are we?” IEEE/ACM, 2024.\\n[61] L. Salewski, S. Alaniz, I. Rio-Torto, E. Schulz, and Z. Akata, “In-context\\nimpersonation reveals large language models’ strengths and biases,”\\narXiv preprint arXiv:2305.14930 , 2023.\\n[62] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y . Cao, Q. Liu, T. Liu, and\\nZ. Sui, “Large language models are not fair evaluators,” arXiv preprint\\narXiv:2305.17926, 2023.\\n[63] Y . Yu, Y . Zhuang, J. Zhang, Y . Meng, A. Ratner, R. Krishna, J. Shen, and\\nC. Zhang, “Large language model as attributed training data generator:\\nA tale of diversity and bias,” arXiv preprint arXiv:2306.15895 , 2023.\\n[64] M. Hernandez, D. R. Avery, S. D. V olpone, and C. R. Kaiser, “Bar-\\ngaining while black: The role of race in salary negotiations.” Journal of\\nApplied Psychology, vol. 104, no. 4, p. 581, 2019.\\n[65] E. O. Arceo-Gomez, R. M. Campos-Vazquez, R. Y . Badillo, and\\nS. Lopez-Araiza, “Gender stereotypes in job advertisements: What do\\nthey imply for the gender salary gap?” Journal of Labor Research ,\\nvol. 43, no. 1, pp. 65–102, 2022.\\n[66] L. L. Taylor, J. N. Lahey, M. I. Beck, and J. E. Froyd, “How to do a\\nsalary equity study: With an illustrative example from higher education,”\\nPublic personnel management , vol. 49, no. 1, pp. 57–82, 2020.\\n[67] J.-P. Platteau and D. U. Ontiveros, “Cognitive bias in insurance: evidence\\nfrom a health scheme in india,” World Development, vol. 144, p. 105498,\\n2021.\\n[68] “Adult income dataset,” www.kaggle.com/datasets/wenruliu/\\nadult-income-dataset, 2023, accessed on August 1, 2023.\\n[69] “Employee dataset,” www.kaggle.com/datasets/tawfikelmetwally/\\nemployee-dataset, 2023, accessed on August 1, 2023.\\n[70] “Us health insurance dataset,” www.kaggle.com/datasets/teertha/\\nushealthinsurancedataset, 2023, accessed on August 1, 2023.\\n[71] N. Mehrabi, F. Morstatter, N. A. Saxena, K. Lerman, and A. G.\\nGalstyan, “A survey on bias and fairness in machine learning,” ACM\\nComputing Surveys (CSUR) , vol. 54, pp. 1 – 35, 2019. [Online].\\nAvailable: https://api.semanticscholar.org/CorpusID:201666566\\n[72] S. Dutta, D. Wei, H. Yueksel, P.-Y . Chen, S. Liu, and K. Varshney, “Is\\nthere a trade-off between fairness and accuracy? a perspective using\\nmismatched hypothesis testing,” in International conference on machine\\nlearning. PMLR, 2020, pp. 2803–2813.\\n[73] P. Barlas, K. Kyriakou, O. Guest, S. Kleanthous, and J. Otterbacher, “To”\\nsee” is to stereotype: Image tagging algorithms, gender recognition, and\\nthe accuracy-fairness trade-off,” Proceedings of the ACM on Human-\\nComputer Interaction, vol. 4, no. CSCW3, pp. 1–31, 2021.\\n[74] Z. Chen, J. Zhang, F. Sarro, and M. Harman, “Fairness improvement\\nwith multiple protected attributes: How far are we?” in 46th Interna-\\ntional Conference on Software Engineering (ICSE 2024) . ACM, 2023.\\n[75] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “Maat: a novel ensem-\\nble approach to addressing fairness and performance bugs for machine\\nlearning software,” in Proceedings of the 30th ACM Joint European\\nSoftware Engineering Conference and Symposium on the Foundations\\nof Software Engineering , 2022, pp. 1122–1134.\\n[76] A. F. Cooper, E. Abrams, and N. Na, “Emergent unfairness in algorith-\\nmic fairness-accuracy trade-off research,” in Proceedings of the 2021\\nAAAI/ACM Conference on AI, Ethics, and Society , 2021, pp. 46–54.\\n[77] S. Liu and L. N. Vicente, “Accuracy and fairness trade-offs in machine\\nlearning: A stochastic multi-objective approach,” Computational Man-\\nagement Science, vol. 19, no. 3, pp. 513–537, 2022.', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 11}),\n",
       " Document(page_content='[78] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,\\nB. Qin, T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained\\nmodel for programming and natural languages,” in Findings of the\\nAssociation for Computational Linguistics: EMNLP 2020 . Online:\\nAssociation for Computational Linguistics, Nov. 2020, pp. 1536–1547.\\n[Online]. Available: https://aclanthology.org/2020.findings-emnlp.139\\n[79] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified\\npre-training for program understanding and generation,” ArXiv, vol.\\nabs/2103.06333, 2021. [Online]. Available: https://api.semanticscholar.\\norg/CorpusID:232185260\\n[80] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y . Wang, W. Chen,\\nand J.-G. Lou, “CERT: Continual pre-training on sketches for library-\\noriented code generation,” in The 2022 International Joint Conference\\non Artificial Intelligence , 2022.\\n[81] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics .\\nPhiladelphia, Pennsylvania, USA: Association for Computational\\nLinguistics, Jul. 2002, pp. 311–318. [Online]. Available: https:\\n//aclanthology.org/P02-1040\\n[82] C.-Y . Lin, “ROUGE: A package for automatic evaluation of summaries,”\\nin Text Summarization Branches Out. Barcelona, Spain: Association for\\nComputational Linguistics, Jul. 2004, pp. 74–81. [Online]. Available:\\nhttps://aclanthology.org/W04-1013\\n[83] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, M. Zhou, A. Blanco,\\nand S. Ma, “Codebleu: a method for automatic evaluation of code\\nsynthesis,” ArXiv, vol. abs/2009.10297, 2020. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:221836101\\n[84] M. Evtikhiev, E. Bogomolov, Y . Sokolov, and T. Bryksin, “Out\\nof the bleu: how should we assess quality of the code generation\\nmodels?” J. Syst. Softw., vol. 203, p. 111741, 2022. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:251371647\\n[85] D. Huang, J. M. Zhang, Y . Qing, and H. Cui, “Effibench: Bench-\\nmarking the efficiency of automatically generated code,” arXiv preprint\\narXiv:2402.02037, 2024.', metadata={'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 167\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (0.27.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: sympy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub) (2024.12.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install huggingface-hub sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load OpenAI API Key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if pinecone_api_key:\n",
    "    print(\"PINECONE API Key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load PINECONE API Key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeGRPC \u001b[38;5;28;01mas\u001b[39;00m Pinecone\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServerlessSpec\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/pinecone/grpc/__init__.py:47\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mConnecting to Pinecone with GRPC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex_grpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GRPCIndex\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeGRPC\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GRPCClientConfig\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/pinecone/grpc/index_grpc.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dict_to_proto_struct, parse_fetch_response, parse_query_response, parse_stats_response\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_factory_grpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorFactoryGRPC\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     FetchResponse,\n\u001b[1;32m     13\u001b[0m     QueryResponse,\n\u001b[1;32m     14\u001b[0m     DescribeIndexStatsResponse,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlist_response\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     ListResponse \u001b[38;5;28;01mas\u001b[39;00m SimpleListResponse,\n\u001b[1;32m     18\u001b[0m     Pagination\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/pinecone/grpc/vector_factory_grpc.py:17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REQUIRED_VECTOR_FIELDS, OPTIONAL_VECTOR_FIELDS\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     VectorDictionaryMissingKeysError, \n\u001b[1;32m     13\u001b[0m     VectorDictionaryExcessKeysError, \n\u001b[1;32m     14\u001b[0m     VectorTupleLengthError, \n\u001b[1;32m     15\u001b[0m     MetadataDictionaryExpectedError \n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse_values_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseValuesFactory\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_service_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Vector \u001b[38;5;28;01mas\u001b[39;00m GRPCVector,\n\u001b[1;32m     21\u001b[0m     SparseValues \u001b[38;5;28;01mas\u001b[39;00m GRPCSparseValues,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     Vector \u001b[38;5;28;01mas\u001b[39;00m NonGRPCVector, \n\u001b[1;32m     25\u001b[0m     SparseValues \u001b[38;5;28;01mas\u001b[39;00m NonGRPCSparseValues\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/pinecone/grpc/sparse_values_factory.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_list\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     SparseValuesTypeError, \n\u001b[1;32m     10\u001b[0m     SparseValuesMissingKeysError, \n\u001b[1;32m     11\u001b[0m     SparseValuesDictionaryExpectedError\n\u001b[1;32m     12\u001b[0m ) \n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_service_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     SparseValues \u001b[38;5;28;01mas\u001b[39;00m GRPCSparseValues,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     SparseValues \u001b[38;5;28;01mas\u001b[39;00m NonGRPCSparseValues\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSparseValuesFactory\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/pinecone/core/grpc/protos/vector_service_pb2.py:41\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprotoc_gen_openapiv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations_pb2 \u001b[38;5;28;01mas\u001b[39;00m protoc__gen__openapiv2_dot_options_dot_annotations__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_service.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m   ,\n\u001b[1;32m     28\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[google_dot_protobuf_dot_struct__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,google_dot_api_dot_annotations__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,google_dot_api_dot_field__behavior__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,protoc__gen__openapiv2_dot_options_dot_annotations__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n\u001b[1;32m     33\u001b[0m _SPARSEVALUES \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     34\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseValues\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseValues\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     38\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m   create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m     40\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 41\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSparseValues.indices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\222\u001b[39;49;00m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;130;43;01m\\036\u001b[39;49;00m\u001b[38;5;124;43mJ\u001b[39;49m\u001b[38;5;130;43;01m\\026\u001b[39;49;00m\u001b[38;5;124;43m[1, 312, 822, 14, 980]x\u001b[39;49m\u001b[38;5;130;43;01m\\350\u001b[39;49;00m\u001b[38;5;130;43;01m\\007\u001b[39;49;00m\u001b[38;5;130;43;01m\\200\u001b[39;49;00m\u001b[38;5;130;43;01m\\001\u001b[39;49;00m\u001b[38;5;130;43;01m\\001\u001b[39;49;00m\u001b[38;5;130;43;01m\\340\u001b[39;49;00m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;130;43;01m\\002\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     48\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     49\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseValues.values\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     50\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     51\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     52\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\222\u001b[39;00m\u001b[38;5;124mA!J\u001b[39m\u001b[38;5;130;01m\\031\u001b[39;00m\u001b[38;5;124m[0.1, 0.2, 0.3, 0.4, 0.5]x\u001b[39m\u001b[38;5;130;01m\\350\u001b[39;00m\u001b[38;5;130;01m\\007\u001b[39;00m\u001b[38;5;130;01m\\200\u001b[39;00m\u001b[38;5;130;01m\\001\u001b[39;00m\u001b[38;5;130;01m\\001\u001b[39;00m\u001b[38;5;130;01m\\340\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;130;01m\\002\u001b[39;00m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,  create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     57\u001b[0m   ],\n\u001b[1;32m     58\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     59\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     60\u001b[0m   ],\n\u001b[1;32m     61\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     64\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     65\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     66\u001b[0m   ],\n\u001b[1;32m     67\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m165\u001b[39m,\n\u001b[1;32m     68\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m291\u001b[39m,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     72\u001b[0m _VECTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     73\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     74\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m547\u001b[39m,\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    125\u001b[0m _SCOREDVECTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m    126\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScoredVector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    127\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScoredVector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m825\u001b[39m,\n\u001b[1;32m    182\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/DocChat/doc_env/lib/python3.9/site-packages/google/protobuf/descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (24.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (3.2.2)\n",
      "Requirement already satisfied: langchain-pinecone==0.1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-pinecone==0.1.1) (0.1.53)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-pinecone==0.1.1) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<4.0.0,>=3.2.2) (2024.12.14)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<4.0.0,>=3.2.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<4.0.0,>=3.2.2) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<4.0.0,>=3.2.2) (2.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (0.1.147)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (2.5.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (8.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (2.14.5)\n",
      "Requirement already satisfied: anyio in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (3.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone==0.1.1) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# First upgrade pip as suggested in the warning\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Then install langchain-pinecone with a compatible pinecone-client version\n",
    "!pip install \"pinecone-client>=3.2.2,<4.0.0\" langchain-pinecone==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a Pinecone vector store\u001b[39;00m\n\u001b[1;32m      4\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m Pinecone\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      5\u001b[0m     documents\u001b[38;5;241m=\u001b[39mtext_chunks,\n\u001b[0;32m----> 6\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[43mindex_name\u001b[49m,\n\u001b[1;32m      7\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_name' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Create a Pinecone vector store\n",
    "docsearch = Pinecone.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (3.2.2)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (2024.12.14)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client) (2.2.3)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: pinecone-client\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 3.2.2\n",
      "    Uninstalling pinecone-client-3.2.2:\n",
      "      Successfully uninstalled pinecone-client-3.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.1.1 requires pinecone-client<4.0.0,>=3.2.2, but you have pinecone-client 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pinecone-client-5.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to index: medicalbot\n"
     ]
    }
   ],
   "source": [
    "# reconnecting the vector db after cutting the connection \n",
    "\n",
    "from pinecone import Pinecone\n",
    "# Initialize the client\n",
    "pc = Pinecone(api_key=\"pcsk_6hSUew_NZ1Mn8bM2QfAjhCZ7twzqB2bXZKSQNd2jnqeF56XSeR2GL5DeJ7m7CZePRbGZvo\")\n",
    "\n",
    "# Connect to an existing index\n",
    "index_name = \"medicalbot\"  # Change from \"example-index\" to \"medicalbot\"\n",
    "try:\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"Connected to index: {index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index\n",
    "from langchain.vectorstores import Pinecone\n",
    "docsearch = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pinecone.Pinecone at 0x7feb78d5d580>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Pinecone', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.pinecone.Pinecone object at 0x7feb78d5d580>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is bias in Quantum?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A−i. The function Func is defined as biased for Ai if, for\\ntwo different values of Ai, say v1 and v2, the output of the\\nfunction changes, while all other parameters in A−i are held\\nconstant. Mathematically, this is represented as:\\nassert Func(A−i, Ai = v1) =Func(A−i, Ai = v2)\\nIn this equation, Func (A−i, Ai = v1) and Func (A−i, Ai =\\nv2) are the outputs of the function Func when Ai takes the\\nvalues v1 and v2 respectively. Code bias exists if the outputs', metadata={'page': 2.0, 'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf'}),\n",
       " Document(page_content='educational-num, marital-status\\nrelationship, race, gender, occupation\\nEmployee [69]\\nEducation, JoiningYear, PaymentTier\\nAge, Gender, Everbenched, LeaveOrNot\\nExperienceInCurrentDomain, City (region)\\nHealth Insurance [70] age, sex (gender) , bmi, children\\nsmoker, region, charges\\nC. Definition of Code Bias\\nInspired by the fairness definition of demographic parity\\n(i.e., the outcome of a model should be independent of pro-\\ntected attributes) in the machine learning literature [71], bias', metadata={'page': 2.0, 'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf'}),\n",
       " Document(page_content='studied bias sensitive tasks in the fairness literature [28], [29],\\n[30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40],\\n[41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51],\\n[52], [53]: adult income related tasks [31], [32], [33], [34],\\n[35], [36], [37], [54] (e.g., to decide whether an adult’s income\\nshould exceed a threshold), employability related tasks [38],\\n[39], [40], [41], [42], [43], [44] (e.g., to decide whether to', metadata={'page': 1.0, 'source': '/Users/mohammad/Downloads/DocChat/data2/bias_testing.pdf'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-pinecone in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (0.1.1)\n",
      "Collecting langchain-pinecone\n",
      "  Downloading langchain_pinecone-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.26 (from langchain)\n",
      "  Using cached langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-pinecone) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2024.12.14)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain-pinecone) (2.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain_pinecone-0.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, pydantic, langchain-core, langchain-pinecone\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.5\n",
      "    Uninstalling pydantic_core-2.14.5:\n",
      "      Successfully uninstalled pydantic_core-2.14.5\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.2\n",
      "    Uninstalling pydantic-2.5.2:\n",
      "      Successfully uninstalled pydantic-2.5.2\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.53\n",
      "    Uninstalling langchain-core-0.1.53:\n",
      "      Successfully uninstalled langchain-core-0.1.53\n",
      "  Attempting uninstall: langchain-pinecone\n",
      "    Found existing installation: langchain-pinecone 0.1.1\n",
      "    Uninstalling langchain-pinecone-0.1.1:\n",
      "      Successfully uninstalled langchain-pinecone-0.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.0.8 requires langchain-core<0.2.0,>=0.1.27, but you have langchain-core 0.3.28 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.3.28 langchain-pinecone-0.2.0 pydantic-2.10.4 pydantic-core-2.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Create a Pinecone vector store\n",
    "docsearch = Pinecone.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: langchain 0.3.13\n",
      "Uninstalling langchain-0.3.13:\n",
      "  Successfully uninstalled langchain-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain-openai langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.3.28)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (1.58.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (23.2)\n",
      "Collecting pydantic<3.0.0,>=2.5.2 (from langchain-core<0.4.0,>=0.3.27->langchain-openai)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
      "Using cached langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Installing collected packages: pydantic, langchain-openai\n",
      "Successfully installed langchain-openai-0.2.14 pydantic-2.10.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-openai\n",
      "Version: 0.2.14\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (0.2.14)\n",
      "Requirement already satisfied: openai in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (1.58.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.3.28)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain-openai 0.2.14\n",
      "Uninstalling langchain-openai-0.2.14:\n",
      "  Successfully uninstalled langchain-openai-0.2.14\n",
      "Found existing installation: pydantic 2.10.4\n",
      "Uninstalling pydantic-2.10.4:\n",
      "  Successfully uninstalled pydantic-2.10.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain-openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.3.28)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (1.58.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
      "Using cached langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Installing collected packages: pydantic, langchain-openai\n",
      "Successfully installed langchain-openai-0.2.14 pydantic-2.10.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.3.13\n",
      "Uninstalling langchain-0.3.13:\n",
      "  Successfully uninstalled langchain-0.3.13\n",
      "Found existing installation: langchain-openai 0.2.14\n",
      "Uninstalling langchain-openai-0.2.14:\n",
      "  Successfully uninstalled langchain-openai-0.2.14\n",
      "Found existing installation: openai 1.58.1\n",
      "Uninstalling openai-1.58.1:\n",
      "  Successfully uninstalled openai-1.58.1\n",
      "Found existing installation: pydantic 2.10.4\n",
      "Uninstalling pydantic-2.10.4:\n",
      "  Successfully uninstalled pydantic-2.10.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall langchain langchain-openai openai pydantic -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain<0.1.0\n",
      "  Downloading langchain-0.0.354-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting openai==0.28.1\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic==1.10.13\n",
      "  Downloading pydantic-1.10.13-cp39-cp39-macosx_10_9_x86_64.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai==0.28.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai==0.28.1) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from openai==0.28.1) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from pydantic==1.10.13) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (2.0.36)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.8 (from langchain<0.1.0)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.5 (from langchain<0.1.0)\n",
      "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain<0.1.0)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (1.26.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain<0.1.0) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.1.0) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.1.0) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.1.0) (3.0.0)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.1,>=0.0.8 (from langchain<0.1.0)\n",
      "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.5 (from langchain<0.1.0)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.5->langchain<0.1.0) (4.7.0)\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain<0.1.0)\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.5->langchain<0.1.0) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.1.0) (3.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain<0.1.0) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain<0.1.0) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.1.0) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp->openai==0.28.1) (0.2.1)\n",
      "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Downloading pydantic-1.10.13-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "Installing collected packages: pydantic, langsmith, langchain-core, openai, langchain-community, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.147\n",
      "    Uninstalling langsmith-0.1.147:\n",
      "      Successfully uninstalled langsmith-0.1.147\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.28\n",
      "    Uninstalling langchain-core-0.3.28:\n",
      "      Successfully uninstalled langchain-core-0.3.28\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.3.13\n",
      "    Uninstalling langchain-community-0.3.13:\n",
      "      Successfully uninstalled langchain-community-0.3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.3.3 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.20 which is incompatible.\n",
      "langchain-experimental 0.3.3 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-pinecone 0.2.0 requires langchain-core<0.4,>=0.3, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-text-splitters 0.3.4 requires langchain-core<0.4.0,>=0.3.26, but you have langchain-core 0.1.23 which is incompatible.\n",
      "pydantic-settings 2.7.0 requires pydantic>=2.7.0, but you have pydantic 1.10.13 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.0.354 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 openai-0.28.1 pydantic-1.10.13\n"
     ]
    }
   ],
   "source": [
    "!pip install \"langchain<0.1.0\" openai==0.28.1 pydantic==1.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(\n",
    "    temperature=0.4,\n",
    "    max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/yb/7l3gtx8n5fb22kdxg9wd88kh0000gn/T/ipykernel_7325/1284054476.py\", line 1, in <module>\n",
      "    response = rag_chain.invoke({\"input\": \"what is bias testing?\"})\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 4525, in invoke\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2499, in invoke\n",
      "    await sequence.ainvoke(1)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py\", line 469, in invoke\n",
      "    # start map output stream\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1626, in _call_with_config\n",
      "    final_input = ichunk\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/config.py\", line 347, in call_func_with_variable_args\n",
      "    func (Union[Callable[[Input], Awaitable[Output]], Callable[[Input,\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py\", line 456, in _invoke\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 3142, in invoke\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 3142, in <dictcomp>\n",
      "  File \"/Users/mohammad/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/mohammad/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/mohammad/opt/anaconda3/lib/python3.9/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 4525, in invoke\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2499, in invoke\n",
      "    await sequence.ainvoke(1)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py\", line 273, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py\", line 568, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py\", line 741, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py\", line 605, in _generate_helper\n",
      "    raise e\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_core/language_models/llms.py\", line 592, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_community/llms/openai.py\", line 460, in _generate\n",
      "    response = completion_with_retry(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_community/llms/openai.py\", line 123, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/Users/mohammad/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/mohammad/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/langchain_community/llms/openai.py\", line 121, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/openai/api_resources/completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/mohammad/Downloads/DocChat/doc_env/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is bias testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
